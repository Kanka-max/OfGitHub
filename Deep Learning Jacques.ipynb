{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08caff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719d1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f3fad",
   "metadata": {},
   "source": [
    "**Layers**\n",
    "- Building block of neural networks.\n",
    "- Filter of data.\n",
    "- Extract representations of data. (More meaningful for the problem at hand)\n",
    "- Implement progressive data distillation.\n",
    "- Also called refined. Data goes in, comes out more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00da466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the MNIST dataset in Keras\n",
    "#comes preloaded in Keras\n",
    "#in form offour Numpy arrays\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d682ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#images encoded as Numpy arrays\n",
    "#labels are an array of digits (0-9)\n",
    "#images and labels have one-to-one corresponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372854cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape\n",
    "len(train_labels)\n",
    "\n",
    "test_images.shape\n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4bf5a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb1f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367293d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4395452f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0493ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe848f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#densely connected\n",
    "#fully connected\n",
    "network.add(layers.Dense(512, activation=\"relu\", input_shape=(28 * 28,)))\n",
    "\n",
    "#10-way softmax layer\n",
    "#Each score will be the probability that the current digit image belongs to\n",
    "#one of our 10 digit classes.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e85388d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compilation step\n",
    "\n",
    "network.compile(optimizer = \"adam\", \n",
    "                loss = \"categorical_crossentropy\", \n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "#loss fx - measure network performance on the training data\n",
    "#optimizer - mechanism through which the network will update itself (data seen & loss fx)\n",
    "#metrics - accuracy-fraction of images correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "393a0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing image data\n",
    "#Reshape data into the shape the network expects and scaling it (0,1)\n",
    "#previously, shape is, (60000, 28, 28,) of type uint8 in the interval [0,255]\n",
    "#Transform it into float32 array of shape:\n",
    "# (60000 * 28 * 28) and intervals (0,1)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66b0bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing labels - categorical encode\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed8473ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d192efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3ebcf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94911777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model to its training data\n",
    "#two quantities displayed during training:\n",
    "#loss of the network over the training data\n",
    "#accuracy of the network over the training data\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on the test set\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"test_acc\": test_acc)\n",
    "\n",
    "#gap between test_acc and train_acc is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a440eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-8c2b92a15af9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m model = Sequential([\n\u001b[0;32m      4\u001b[0m     \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.model'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a76b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Tensor slicing.\n",
    "\n",
    "#selecting a specific digit alongside the first axis\n",
    "# syntax\n",
    "\n",
    "#train_images[i]\n",
    "\n",
    "#select digit #10 to #100 not including #100\n",
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)\n",
    "\n",
    "#puts them in this array shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc70d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#equivalent to\n",
    "#specifies a start and stop index\n",
    "\n",
    "my_second_slice = train_images[10:100, :, :]\n",
    "print(my_second_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cab9f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also\n",
    "\n",
    "myThirdSlice = train_images[10:100, 0:28, 0:28]\n",
    "myThirdSlice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaba6062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 14, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you may select between any two indices along each tensor axis\n",
    "#in order to select 14 X 14 pixels in the bottom-right corner of all images\n",
    "\n",
    "my_slice = train_images[:, 14: , 14:]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2edfb467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 14, 14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicate a positive relative to the end of the current axis.\n",
    "# in order to crop the images to patches of 14 X 14 pixels centered in the middle.\n",
    "\n",
    "my_centered_slice = train_images[:, 7:-7, 7:-7]\n",
    "my_centered_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0cad4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The notion of batches\n",
    "\n",
    "batch = train_images[:128]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8558fd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next batch\n",
    "\n",
    "batch = train_images[128:256]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e243bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nth batch\n",
    "\n",
    "batch = train_images[128* n: 128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68607738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f063afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(26)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(26)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a5548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a170f10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26,  17, 100,  45,  68])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([26, 17, 100, 45, 68])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4da974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21802eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74, 69,  0, 55, 11],\n",
       "       [96,  4, 19, 21,  6],\n",
       "       [10, 88,  9, 30, 44]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[74, 69, 0, 55, 11], \n",
    "              [96, 4, 19, 21, 6], \n",
    "              [10, 88, 9, 30, 44]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55a4314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb1bca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]]])\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647827c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
